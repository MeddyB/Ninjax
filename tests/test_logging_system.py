"""
Tests pour le syst√®me de logging complet
"""
import os
import sys
import tempfile
import json
import time
from pathlib import Path
from unittest.mock import patch, MagicMock

# Ajouter le r√©pertoire src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from core.config import Config
from core.logging_config import (
    setup_logging, 
    get_metrics_collector, 
    log_performance,
    log_function_call,
    MetricsCollector,
    PerformanceMetric
)
from utils.monitoring import (
    SystemMonitor,
    PerformanceTracker,
    HealthChecker,
    create_monitoring_report
)


def test_basic_logging():
    """Test du logging de base"""
    print("Test du logging de base...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # Configuration de test
        config = Config()
        config.LOG_FILE = os.path.join(temp_dir, "test.log")
        config.LOG_LEVEL = "DEBUG"
        config.ENVIRONMENT = "development"
        
        # Configurer le logging
        logger = setup_logging(config, "test_logger")
        
        # Tester diff√©rents niveaux
        logger.debug("Message de debug")
        logger.info("Message d'info")
        logger.warning("Message de warning")
        logger.error("Message d'erreur")
        
        # V√©rifier que le fichier de log existe
        assert os.path.exists(config.LOG_FILE), "Le fichier de log n'a pas √©t√© cr√©√©"
        
        # V√©rifier le contenu
        with open(config.LOG_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
            assert "Message de debug" in content
            assert "Message d'info" in content
            assert "Message de warning" in content
            assert "Message d'erreur" in content
        
        print("‚úÖ Test du logging de base r√©ussi")


def test_metrics_collection():
    """Test de la collecte de m√©triques"""
    print("Test de la collecte de m√©triques...")
    
    # Cr√©er un collecteur de m√©triques
    collector = MetricsCollector()
    
    # Enregistrer quelques m√©triques
    collector.record_log("INFO")
    collector.record_log("ERROR")
    collector.record_log("DEBUG")
    
    # Enregistrer une m√©trique de performance
    from datetime import datetime
    metric = PerformanceMetric(
        operation="test_operation",
        duration=1.5,
        timestamp=datetime.utcnow(),
        success=True
    )
    collector.record_performance(metric)
    
    # V√©rifier les m√©triques
    summary = collector.get_metrics_summary()
    
    assert summary['total_logs'] == 3, f"Expected 3 logs, got {summary['total_logs']}"
    assert summary['errors_count'] == 1, f"Expected 1 error, got {summary['errors_count']}"
    assert summary['logs_by_level']['INFO'] == 1
    assert summary['logs_by_level']['ERROR'] == 1
    assert summary['logs_by_level']['DEBUG'] == 1
    
    print("‚úÖ Test de la collecte de m√©triques r√©ussi")


def test_performance_logging():
    """Test du logging de performance"""
    print("Test du logging de performance...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        config = Config()
        config.LOG_FILE = os.path.join(temp_dir, "test_perf.log")
        config.LOG_LEVEL = "DEBUG"
        
        logger = setup_logging(config, "test_perf_logger")
        
        # Test du context manager de performance
        with log_performance(logger, "test_operation", {"param": "value"}):
            time.sleep(0.1)  # Simuler une op√©ration
        
        # Test du d√©corateur de fonction
        @log_function_call(logger, include_args=True)
        def test_function(x, y=10):
            time.sleep(0.05)
            return x + y
        
        result = test_function(5, y=15)
        assert result == 20
        
        # V√©rifier les logs
        with open(config.LOG_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
            assert "test_operation" in content
            assert "test_function" in content
        
        print("‚úÖ Test du logging de performance r√©ussi")


def test_system_monitoring():
    """Test du monitoring syst√®me"""
    print("Test du monitoring syst√®me...")
    
    try:
        monitor = SystemMonitor()
        
        # Test des m√©triques syst√®me
        metrics = monitor.get_system_metrics()
        
        assert 0 <= metrics.cpu_percent <= 100
        assert 0 <= metrics.memory_percent <= 100
        assert metrics.memory_used_mb > 0
        assert metrics.disk_free_gb >= 0
        assert metrics.process_count > 0
        assert metrics.uptime_seconds >= 0
        
        # Test des m√©triques de processus
        process_metrics = monitor.get_process_metrics()
        
        if 'error' not in process_metrics:
            assert process_metrics['pid'] > 0
            assert process_metrics['memory_rss_mb'] > 0
            assert process_metrics['num_threads'] > 0
        
        print("‚úÖ Test du monitoring syst√®me r√©ussi")
        
    except ImportError:
        print("‚ö†Ô∏è  psutil non disponible, test du monitoring syst√®me ignor√©")


def test_performance_tracker():
    """Test du tracker de performance"""
    print("Test du tracker de performance...")
    
    try:
        tracker = PerformanceTracker()
        
        # D√©marrer une op√©ration
        op_id = tracker.start_operation("test_tracking")
        
        # Simuler une op√©ration
        time.sleep(0.1)
        
        # Terminer l'op√©ration
        metrics = tracker.end_operation(op_id, success=True)
        
        assert metrics['operation_name'] == "test_tracking"
        assert metrics['duration'] >= 0.1
        assert metrics['success'] is True
        assert 'timestamp' in metrics
        
        print("‚úÖ Test du tracker de performance r√©ussi")
        
    except ImportError:
        print("‚ö†Ô∏è  psutil non disponible, test du tracker de performance ignor√©")


def test_health_checker():
    """Test du v√©rificateur de sant√©"""
    print("Test du v√©rificateur de sant√©...")
    
    health_checker = HealthChecker()
    
    # Ajouter une v√©rification simple
    health_checker.register_check(
        "always_pass",
        lambda: True,
        None
    )
    
    health_checker.register_check(
        "threshold_test",
        lambda: 50,
        100  # Seuil de 100, donc 50 devrait passer
    )
    
    # Ex√©cuter les v√©rifications
    results = health_checker.run_health_checks()
    
    assert 'overall_status' in results
    assert 'timestamp' in results
    assert 'checks' in results
    assert 'system_metrics' in results
    
    # V√©rifier les r√©sultats des checks
    assert results['checks']['always_pass']['status'] == 'pass'
    assert results['checks']['threshold_test']['status'] == 'pass'
    
    print("‚úÖ Test du v√©rificateur de sant√© r√©ussi")


def test_monitoring_report():
    """Test de la g√©n√©ration de rapport de monitoring"""
    print("Test de la g√©n√©ration de rapport de monitoring...")
    
    try:
        report = create_monitoring_report()
        
        # V√©rifier la structure du rapport
        assert 'timestamp' in report
        assert 'system_metrics' in report
        assert 'process_metrics' in report
        assert 'health_check' in report
        assert 'log_metrics' in report
        
        # V√©rifier que le timestamp est valide
        from datetime import datetime
        timestamp = datetime.fromisoformat(report['timestamp'].replace('Z', '+00:00'))
        assert timestamp is not None
        
        print("‚úÖ Test de la g√©n√©ration de rapport de monitoring r√©ussi")
        
    except ImportError:
        print("‚ö†Ô∏è  psutil non disponible, test de rapport de monitoring ignor√©")


def test_json_export():
    """Test de l'export JSON des m√©triques"""
    print("Test de l'export JSON des m√©triques...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        collector = MetricsCollector()
        
        # Ajouter quelques m√©triques
        collector.record_log("INFO")
        collector.record_log("ERROR")
        
        # Exporter
        export_file = os.path.join(temp_dir, "test_export.json")
        collector.export_metrics(export_file)
        
        # V√©rifier l'export
        assert os.path.exists(export_file)
        
        with open(export_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            assert 'export_time' in data
            assert 'summary' in data
            assert 'performance_history' in data
            
            summary = data['summary']
            assert summary['total_logs'] == 2
            assert summary['errors_count'] == 1
        
        print("‚úÖ Test de l'export JSON des m√©triques r√©ussi")


def run_all_tests():
    """Ex√©cute tous les tests"""
    print("D√©marrage des tests du syst√®me de logging...")
    print("=" * 50)
    
    tests = [
        test_basic_logging,
        test_metrics_collection,
        test_performance_logging,
        test_system_monitoring,
        test_performance_tracker,
        test_health_checker,
        test_monitoring_report,
        test_json_export
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test()
            passed += 1
        except Exception as e:
            print(f"‚ùå {test.__name__} √©chou√©: {e}")
            failed += 1
        print()
    
    print("=" * 50)
    print(f"R√©sultats: {passed} r√©ussis, {failed} √©chou√©s")
    
    if failed == 0:
        print("üéâ Tous les tests sont pass√©s!")
        return True
    else:
        print("‚ö†Ô∏è  Certains tests ont √©chou√©")
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)